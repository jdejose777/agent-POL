  {
    "nodes": [
      {
        "parameters": {
          "path": "rag-chat-agent",
          "options": {
            "noResponseBody": false
          }
        },
        "type": "n8n-nodes-base.webhook",
        "typeVersion": 2.1,
        "position": [
          0,
          300
        ],
        "id": "ca7cf43e-2a8a-47de-91e6-eebcfde53362",
        "name": "üéØ Webhook - Entrada del Chat",
        "notes": "Punto de entrada para las preguntas del usuario desde el frontend"
      },
      {
        "parameters": {
          "mode": "runOnceForAllItems",
          "pythonCode": "# üîß NODO 2: GENERAR EMBEDDING DE LA PREGUNTA\n# Este nodo toma la pregunta del usuario y genera su embedding usando sentence-transformers local\n\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom datetime import datetime\n\n# Obtener la pregunta del webhook desde el item anterior\npregunta = items[0].json[\"body\"][\"pregunta\"]\n\n# Validar que se recibi√≥ una pregunta\nif not pregunta:\n    raise ValueError(\"‚ùå No se recibi√≥ una pregunta v√°lida\")\n\nprint(f\"üìù Pregunta recibida: {pregunta}\")\n\n# Cargar el modelo sentence-transformers\n# Usando el mismo modelo que se us√≥ para generar los embeddings en Pinecone\nprint(\"ü§ñ Cargando modelo all-MiniLM-L6-v2...\")\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generar el embedding de la pregunta\nprint(\"üîÑ Generando embedding...\")\nembedding_array = model.encode([pregunta])[0]\n\n# Convertir numpy array a lista para compatibilidad JSON\nembedding_list = embedding_array.tolist()\n\nprint(f\"‚úÖ Embedding generado correctamente\")\nprint(f\"üìä Dimensiones del embedding: {len(embedding_list)}\")\nprint(f\"üéØ Tipo de datos: {type(embedding_list)}\")\n\n# Devolver los datos para el siguiente nodo en formato n8n\n# El formato debe ser una lista de diccionarios con la clave 'json'\nreturn [\n    {\n        \"json\": {\n            \"pregunta\": pregunta,\n            \"embedding\": embedding_list,\n            \"timestamp\": datetime.now().isoformat(),\n            \"dimensiones\": len(embedding_list),\n            \"modelo\": \"all-MiniLM-L6-v2\"\n        }\n    }\n]"
        },
        "type": "n8n-nodes-base.code",
        "typeVersion": 2,
        "position": [
          240,
          300
        ],
        "id": "embed-generator-node",
        "name": "üß† Generar Embedding Python",
        "notes": "Convierte la pregunta del usuario en embedding usando sentence-transformers local"
      },
      {
        "parameters": {
          "jsCode": "// üîç NODO 3: B√öSQUEDA EN PINECONE\n// Este nodo busca contenido relevante en la base de datos vectorial usando el embedding\n\n// Obtener datos del nodo anterior\nconst { pregunta, embedding } = $input.first().json;\n\nconsole.log('üîç Iniciando b√∫squeda en Pinecone para:', pregunta);\n\n// üîê Obtener configuraci√≥n de Pinecone desde variables de workflow\n// IMPORTANTE: Configura estas variables en: Workflow Settings ‚Üí Variables\nconst pineconeApiKey = $vars.PINECONE_API_KEY;\nconst pineconeEnvironment = $vars.PINECONE_ENVIRONMENT;\nconst pineconeIndexName = $vars.PINECONE_INDEX_NAME;\n\n// Validar que las variables est√©n configuradas\nif (!pineconeApiKey || !pineconeEnvironment || !pineconeIndexName) {\n  throw new Error('‚ùå Variables de Pinecone no configuradas. Ve a Workflow Settings ‚Üí Variables y configura: PINECONE_API_KEY, PINECONE_ENVIRONMENT, PINECONE_INDEX_NAME');\n}\n\nconsole.log('‚úÖ Credenciales de Pinecone cargadas correctamente');\nconsole.log(`üìç Usando √≠ndice: ${pineconeIndexName} en environment: ${pineconeEnvironment}`);\n\n// Construir URL de Pinecone\nconst pineconeUrl = `https://${pineconeIndexName}-${pineconeEnvironment}.svc.pinecone.io/query`;\n\n// Funci√≥n para buscar en Pinecone\nasync function buscarEnPinecone(queryEmbedding, topK = 5) {\n  const response = await $http.request({\n    method: 'POST',\n    url: pineconeUrl,\n    headers: {\n      'Api-Key': pineconeApiKey,\n      'Content-Type': 'application/json'\n    },\n    body: {\n      vector: queryEmbedding,\n      topK: topK,\n      includeMetadata: true,\n      includeValues: false\n    }\n  });\n  \n  if (response.status !== 200) {\n    throw new Error(`‚ùå Error en Pinecone API: ${response.status} - ${response.statusText}`);\n  }\n  \n  return response.data;\n}\n\n// Realizar b√∫squeda\nconst resultadosBusqueda = await buscarEnPinecone(embedding, 5);\n\n// Extraer contexto relevante\nconst matches = resultadosBusqueda.matches || [];\nconst contexto = matches\n  .filter(match => match.score > 0.7) // Solo matches con alta similitud\n  .map(match => match.metadata?.text || '')\n  .filter(text => text.length > 0)\n  .join('\\n\\n');\n\nconsole.log('üìã Contexto encontrado:');\nconsole.log(`   - Matches encontrados: ${matches.length}`);\nconsole.log(`   - Matches relevantes: ${matches.filter(m => m.score > 0.7).length}`);\nconsole.log(`   - Longitud del contexto: ${contexto.length} caracteres`);\n\nif (contexto.length === 0) {\n  console.log('‚ö†Ô∏è  No se encontr√≥ contexto relevante');\n}\n\n// Devolver datos para el siguiente nodo\nreturn {\n  pregunta: pregunta,\n  contexto: contexto,\n  matches: matches,\n  numeroResultados: matches.length,\n  tieneContexto: contexto.length > 0\n};"
        },
        "type": "n8n-nodes-base.code",
        "typeVersion": 2,
        "position": [
          480,
          300
        ],
        "id": "pinecone-search-node",
        "name": "üîç B√∫squeda en Pinecone",
        "notes": "Busca informaci√≥n relevante en la base de datos vectorial"
      },
      {
        "parameters": {
          "method": "POST",
          "url": "URL_DE_LA_API_AQUI",
          "authentication": "headerAuth",
          "headerAuth": {
            "name": "Authorization",
            "value": "Bearer {{ $credentials.miApiKeySecreta }}"
          },
          "sendBody": true,
          "contentType": "json",
          "bodyParameters": {
            "parameters": [
              {
                "name": "model",
                "value": "llama3-8b-8192"
              },
              {
                "name": "messages",
                "value": "=[{\n  \"role\": \"user\",\n  \"content\": \"Eres un asistente jur√≠dico especializado en el C√≥digo Penal espa√±ol. Responde bas√°ndote √∫nicamente en el contexto proporcionado del C√≥digo Penal.\\n\\n{{ $json.tieneContexto ? 'CONTEXTO DEL C√ìDIGO PENAL:\\n' + $json.contexto + '\\n\\n' : 'No se encontr√≥ informaci√≥n espec√≠fica en el C√≥digo Penal para esta consulta.\\n\\n' }}PREGUNTA:\\n{{ $json.pregunta }}\\n\\nINSTRUCCIONES:\\n- Responde bas√°ndote √∫nicamente en el contexto del C√≥digo Penal proporcionado\\n- Si la informaci√≥n no est√° en el contexto, ind√≠calo claramente\\n- Cita los art√≠culos espec√≠ficos cuando sea posible\\n- Usa un lenguaje claro y profesional\\n- Si es relevante, menciona las penas asociadas\\n\\nRESPUESTA:\"\n}]"
              },
              {
                "name": "temperature",
                "value": "0.3"
              }
            ]
          },
          "options": {
            "response": {
              "response": {
                "neverError": false,
                "responseFormat": "autodetect",
                "outputPropertyName": "data"
              }
            }
          }
        },
        "type": "n8n-nodes-base.httpRequest",
        "typeVersion": 4.2,
        "position": [
          720,
          300
        ],
        "id": "llm-response-node",
        "name": "ü§ñ LLM Open Source",
        "notes": "Genera respuesta especializada usando LLM de c√≥digo abierto (Groq/HuggingFace/Ollama)"
      },
      {
        "parameters": {
          "jsCode": "// üîß NODO 5: PROCESAR RESPUESTA DEL LLM\n// Este nodo extrae la respuesta del LLM y la formatea para el frontend\n\n// Obtener datos del nodo anterior\nconst llmResponse = $input.first().json;\nconst preguntaOriginal = $input.first().json.pregunta || 'Pregunta no disponible';\nconst tieneContexto = $input.first().json.tieneContexto || false;\nconst numeroResultados = $input.first().json.numeroResultados || 0;\n\nconsole.log('üîß Procesando respuesta del LLM...');\nconsole.log('üìä Datos recibidos:', Object.keys(llmResponse));\n\n// Extraer la respuesta del LLM seg√∫n el formato de respuesta\nlet respuestaFinal = '';\n\n// Intentar extraer la respuesta de diferentes formatos de API\nif (llmResponse.choices && llmResponse.choices[0]) {\n  // Formato OpenAI/Groq\n  respuestaFinal = llmResponse.choices[0].message?.content || llmResponse.choices[0].text || '';\n} else if (llmResponse.generated_text) {\n  // Formato Hugging Face\n  respuestaFinal = llmResponse.generated_text;\n} else if (llmResponse.response) {\n  // Formato Ollama\n  respuestaFinal = llmResponse.response;\n} else if (llmResponse.message) {\n  // Formato gen√©rico\n  respuestaFinal = llmResponse.message.content || llmResponse.message;\n} else if (typeof llmResponse.data === 'string') {\n  // Respuesta directa como string\n  respuestaFinal = llmResponse.data;\n} else if (llmResponse.content) {\n  // Formato directo\n  respuestaFinal = llmResponse.content;\n} else {\n  // Fallback: intentar encontrar cualquier campo que contenga texto\n  const possibleFields = ['text', 'output', 'result', 'answer'];\n  for (const field of possibleFields) {\n    if (llmResponse[field] && typeof llmResponse[field] === 'string') {\n      respuestaFinal = llmResponse[field];\n      break;\n    }\n  }\n}\n\n// Si no se encontr√≥ respuesta, usar toda la respuesta como string\nif (!respuestaFinal && typeof llmResponse === 'string') {\n  respuestaFinal = llmResponse;\n} else if (!respuestaFinal) {\n  respuestaFinal = 'Lo siento, no pude procesar la respuesta del modelo de lenguaje.';\n}\n\n// Limpiar la respuesta\nrespuestaFinal = respuestaFinal.trim();\n\nconsole.log('‚úÖ Respuesta procesada correctamente');\nconsole.log('üìù Longitud de la respuesta:', respuestaFinal.length, 'caracteres');\n\n// Preparar respuesta final para el webhook\nconst respuestaCompleta = {\n  respuesta: respuestaFinal,\n  metadata: {\n    pregunta: preguntaOriginal,\n    tieneContexto: tieneContexto,\n    numeroResultados: numeroResultados,\n    timestamp: new Date().toISOString(),\n    modelo: 'llm-open-source',\n    dominio: 'codigo-penal-espanol'\n  }\n};\n\nreturn respuestaCompleta;"
        },
        "type": "n8n-nodes-base.code",
        "typeVersion": 2,
        "position": [
          880,
          300
        ],
        "id": "process-llm-response-node",
        "name": "üîß Procesar Respuesta LLM",
        "notes": "Extrae y formatea la respuesta del LLM para el frontend"
      },
      {
        "parameters": {
          "respondWith": "json",
          "responseBody": "={{ $json }}",
          "options": {
            "responseHeaders": {
              "entries": [
                {
                  "name": "Content-Type",
                  "value": "application/json"
                },
                {
                  "name": "Access-Control-Allow-Origin",
                  "value": "*"
                },
                {
                  "name": "Access-Control-Allow-Headers",
                  "value": "Content-Type"
                }
              ]
            }
          }
        },
        "type": "n8n-nodes-base.respondToWebhook",
        "typeVersion": 1.4,
        "position": [
          1040,
          300
        ],
        "id": "c6b648a2-d9c0-4860-80cd-5e623175a027",
        "name": "üì§ Responder al Chat",
        "notes": "Devuelve la respuesta final al frontend con headers CORS"
      }
    ],
    "connections": {
      "üéØ Webhook - Entrada del Chat": {
        "main": [
          [
            {
              "node": "üß† Generar Embedding Python",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "üß† Generar Embedding Python": {
        "main": [
          [
            {
              "node": "üîç B√∫squeda en Pinecone",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "üîç B√∫squeda en Pinecone": {
        "main": [
          [
            {
              "node": "ü§ñ LLM Open Source",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "ü§ñ LLM Open Source": {
        "main": [
          [
            {
              "node": "üîß Procesar Respuesta LLM",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "üîß Procesar Respuesta LLM": {
        "main": [
          [
            {
              "node": "üì§ Responder al Chat",
              "type": "main",
              "index": 0
            }
          ]
        ]
      }
    },
    "pinData": {},
    "meta": {
      "templateCreationSource": "n8n_agent_rag_chat",
      "instanceId": "8445ee8fb1232b8fb10ebac704619d5ec8c5d77a6bf0cb1d173277b89a3cde54",
      "description": "‚öñÔ∏è Asistente jur√≠dico especializado en C√≥digo Penal espa√±ol. Usa embeddings locales y genera respuestas contextuales con Gemini.",
      "name": "Agent POL - Asistente Jur√≠dico"
    }
  }