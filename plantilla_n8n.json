{
  "nodes": [
    {
      "parameters": {
        "path": "rag-chat-agent",
        "options": {
          "noResponseBody": false
        }
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2.1,
      "position": [
        0,
        300
      ],
      "id": "ca7cf43e-2a8a-47de-91e6-eebcfde53362",
      "name": "üéØ Webhook - Entrada del Chat",
      "notes": "Punto de entrada para las preguntas del usuario desde el frontend"
    },
    {
      "parameters": {
        "jsCode": "// üîß NODO 2: GENERAR EMBEDDING DE LA PREGUNTA\n// Este nodo toma la pregunta del usuario y genera su embedding usando OpenAI\n\n// Obtener la pregunta del webhook\nconst pregunta = $input.first().json.body.pregunta;\n\nif (!pregunta) {\n  throw new Error('‚ùå No se recibi√≥ una pregunta v√°lida');\n}\n\nconsole.log('üìù Pregunta recibida:', pregunta);\n\n// Configuraci√≥n de OpenAI\nconst openaiApiKey = $env.OPENAI_API_KEY;\nif (!openaiApiKey) {\n  throw new Error('‚ùå OPENAI_API_KEY no est√° configurada en las variables de entorno');\n}\n\n// Funci√≥n para generar embedding\nasync function generarEmbedding(texto) {\n  const response = await $http.request({\n    method: 'POST',\n    url: 'https://api.openai.com/v1/embeddings',\n    headers: {\n      'Authorization': `Bearer ${openaiApiKey}`,\n      'Content-Type': 'application/json'\n    },\n    body: {\n      model: 'text-embedding-3-small',\n      input: texto\n    }\n  });\n  \n  if (response.status !== 200) {\n    throw new Error(`‚ùå Error en OpenAI API: ${response.status} - ${response.statusText}`);\n  }\n  \n  return response.data.data[0].embedding;\n}\n\n// Generar el embedding\nconst embedding = await generarEmbedding(pregunta);\n\nconsole.log('‚úÖ Embedding generado correctamente');\nconsole.log('üìä Dimensiones del embedding:', embedding.length);\n\n// Devolver los datos para el siguiente nodo\nreturn {\n  pregunta: pregunta,\n  embedding: embedding,\n  timestamp: new Date().toISOString()\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        240,
        300
      ],
      "id": "embed-generator-node",
      "name": "üß† Generar Embedding",
      "notes": "Convierte la pregunta del usuario en embedding usando OpenAI"
    },
    {
      "parameters": {
        "jsCode": "// üîç NODO 3: B√öSQUEDA EN PINECONE\n// Este nodo busca contenido relevante en la base de datos vectorial usando el embedding\n\n// Obtener datos del nodo anterior\nconst { pregunta, embedding } = $input.first().json;\n\nconsole.log('üîç Iniciando b√∫squeda en Pinecone para:', pregunta);\n\n// Configuraci√≥n de Pinecone\nconst pineconeApiKey = $env.PINECONE_API_KEY;\nconst pineconeEnvironment = $env.PINECONE_ENVIRONMENT;\nconst pineconeIndexName = $env.PINECONE_INDEX_NAME;\n\nif (!pineconeApiKey || !pineconeEnvironment || !pineconeIndexName) {\n  throw new Error('‚ùå Variables de Pinecone no configuradas: PINECONE_API_KEY, PINECONE_ENVIRONMENT, PINECONE_INDEX_NAME');\n}\n\n// Construir URL de Pinecone\nconst pineconeUrl = `https://${pineconeIndexName}-${pineconeEnvironment}.svc.pinecone.io/query`;\n\n// Funci√≥n para buscar en Pinecone\nasync function buscarEnPinecone(queryEmbedding, topK = 5) {\n  const response = await $http.request({\n    method: 'POST',\n    url: pineconeUrl,\n    headers: {\n      'Api-Key': pineconeApiKey,\n      'Content-Type': 'application/json'\n    },\n    body: {\n      vector: queryEmbedding,\n      topK: topK,\n      includeMetadata: true,\n      includeValues: false\n    }\n  });\n  \n  if (response.status !== 200) {\n    throw new Error(`‚ùå Error en Pinecone API: ${response.status} - ${response.statusText}`);\n  }\n  \n  return response.data;\n}\n\n// Realizar b√∫squeda\nconst resultadosBusqueda = await buscarEnPinecone(embedding, 5);\n\n// Extraer contexto relevante\nconst matches = resultadosBusqueda.matches || [];\nconst contexto = matches\n  .filter(match => match.score > 0.7) // Solo matches con alta similitud\n  .map(match => match.metadata?.text || '')\n  .filter(text => text.length > 0)\n  .join('\\n\\n');\n\nconsole.log('üìã Contexto encontrado:');\nconsole.log(`   - Matches encontrados: ${matches.length}`);\nconsole.log(`   - Matches relevantes: ${matches.filter(m => m.score > 0.7).length}`);\nconsole.log(`   - Longitud del contexto: ${contexto.length} caracteres`);\n\nif (contexto.length === 0) {\n  console.log('‚ö†Ô∏è  No se encontr√≥ contexto relevante');\n}\n\n// Devolver datos para el siguiente nodo\nreturn {\n  pregunta: pregunta,\n  contexto: contexto,\n  matches: matches,\n  numeroResultados: matches.length,\n  tieneContexto: contexto.length > 0\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        480,
        300
      ],
      "id": "pinecone-search-node",
      "name": "üîç B√∫squeda en Pinecone",
      "notes": "Busca informaci√≥n relevante en la base de datos vectorial"
    },
    {
      "parameters": {
        "jsCode": "// ü§ñ NODO 4: GENERAR RESPUESTA CON OPENAI\n// Este nodo construye el prompt final y genera la respuesta usando GPT\n\n// Obtener datos del nodo anterior\nconst { pregunta, contexto, tieneContexto, numeroResultados } = $input.first().json;\n\nconsole.log('ü§ñ Generando respuesta para:', pregunta);\nconsole.log('üìö Contexto disponible:', tieneContexto);\n\n// Configuraci√≥n de OpenAI\nconst openaiApiKey = $env.OPENAI_API_KEY;\nif (!openaiApiKey) {\n  throw new Error('‚ùå OPENAI_API_KEY no est√° configurada');\n}\n\n// Construir el prompt seg√∫n si hay contexto o no\nlet promptFinal;\n\nif (tieneContexto) {\n  promptFinal = `Eres un asistente inteligente que responde preguntas bas√°ndose en documentos procesados.\n\nCONTEXTO RELEVANTE:\n${contexto}\n\nPREGUNTA DEL USUARIO:\n${pregunta}\n\nINSTRUCCIONES:\n- Responde √∫nicamente bas√°ndote en el contexto proporcionado\n- Si la informaci√≥n no est√° en el contexto, di que no tienes esa informaci√≥n espec√≠fica\n- S√© claro, conciso y √∫til\n- Usa un tono profesional pero amigable\n- Si es apropiado, cita partes relevantes del documento\n\nRESPUESTA:`;\n} else {\n  promptFinal = `Eres un asistente inteligente para un sistema RAG (Retrieval-Augmented Generation).\n\nPREGUNTA DEL USUARIO:\n${pregunta}\n\nNo se encontr√≥ informaci√≥n relevante en los documentos procesados para responder a esta pregunta espec√≠fica.\n\nPor favor, responde de manera educada explicando que:\n1. No se encontr√≥ informaci√≥n relevante en los documentos disponibles\n2. Sugiere reformular la pregunta o verificar que el documento correcto haya sido procesado\n3. Mant√©n un tono profesional pero amigable\n\nRESPUESTA:`;\n}\n\n// Funci√≥n para llamar a OpenAI GPT\nasync function generarRespuestaGPT(prompt) {\n  const response = await $http.request({\n    method: 'POST',\n    url: 'https://api.openai.com/v1/chat/completions',\n    headers: {\n      'Authorization': `Bearer ${openaiApiKey}`,\n      'Content-Type': 'application/json'\n    },\n    body: {\n      model: 'gpt-4o-mini', // Modelo eficiente y econ√≥mico\n      messages: [\n        {\n          role: 'user',\n          content: prompt\n        }\n      ],\n      max_tokens: 500,\n      temperature: 0.7,\n      top_p: 1,\n      frequency_penalty: 0,\n      presence_penalty: 0\n    }\n  });\n  \n  if (response.status !== 200) {\n    throw new Error(`‚ùå Error en OpenAI API: ${response.status} - ${response.statusText}`);\n  }\n  \n  return response.data.choices[0].message.content.trim();\n}\n\n// Generar la respuesta\nconst respuestaFinal = await generarRespuestaGPT(promptFinal);\n\nconsole.log('‚úÖ Respuesta generada correctamente');\nconsole.log('üìù Longitud de la respuesta:', respuestaFinal.length, 'caracteres');\n\n// Preparar respuesta final para el webhook\nconst respuestaCompleta = {\n  respuesta: respuestaFinal,\n  metadata: {\n    pregunta: pregunta,\n    tieneContexto: tieneContexto,\n    numeroResultados: numeroResultados,\n    timestamp: new Date().toISOString(),\n    modelo: 'gpt-4o-mini'\n  }\n};\n\nreturn respuestaCompleta;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        720,
        300
      ],
      "id": "openai-response-node",
      "name": "ü§ñ Generar Respuesta GPT",
      "notes": "Crea el prompt final y genera la respuesta usando OpenAI"
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {
          "responseHeaders": {
            "entries": [
              {
                "name": "Content-Type",
                "value": "application/json"
              },
              {
                "name": "Access-Control-Allow-Origin",
                "value": "*"
              },
              {
                "name": "Access-Control-Allow-Headers",
                "value": "Content-Type"
              }
            ]
          }
        }
      },
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.4,
      "position": [
        960,
        300
      ],
      "id": "c6b648a2-d9c0-4860-80cd-5e623175a027",
      "name": "üì§ Responder al Chat",
      "notes": "Devuelve la respuesta final al frontend con headers CORS"
    }
  ],
  "connections": {
    "üéØ Webhook - Entrada del Chat": {
      "main": [
        [
          {
            "node": "üß† Generar Embedding",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "üß† Generar Embedding": {
      "main": [
        [
          {
            "node": "üîç B√∫squeda en Pinecone",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "üîç B√∫squeda en Pinecone": {
      "main": [
        [
          {
            "node": "ü§ñ Generar Respuesta GPT",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ü§ñ Generar Respuesta GPT": {
      "main": [
        [
          {
            "node": "üì§ Responder al Chat",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "pinData": {},
  "meta": {
    "templateCreationSource": "n8n_agent_rag_chat",
    "instanceId": "8445ee8fb1232b8fb10ebac704619d5ec8c5d77a6bf0cb1d173277b89a3cde54",
    "description": "ü§ñ Agente RAG completo para chat inteligente con documentos PDF. Procesa preguntas, busca en Pinecone y genera respuestas contextuales.",
    "name": "Agent POL - RAG Chat System"
  }
}