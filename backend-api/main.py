# main.py
# Backend API para el sistema RAG de consultas legales
# Versi√≥n con Vertex AI (Google Cloud)

import os
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

# --- IMPORTS ADICIONALES PARA RAG CON VERTEX AI ---
from google.cloud import aiplatform
from vertexai.language_models import TextEmbeddingModel
from vertexai.generative_models import GenerativeModel
from pinecone import Pinecone
import vertexai

# Cargar las variables de entorno desde .env
load_dotenv()

# --- 1. CONFIGURACI√ìN DE VERTEX AI Y PINECONE ---
PROJECT_ID = os.getenv("GCP_PROJECT_ID", "resolute-return-476416-g5")
REGION = os.getenv("GCP_REGION", "us-central1")
MODEL_NAME = "gemini-2.0-flash-001"  # Modelo de generaci√≥n
EMBEDDING_MODEL = "text-embedding-004"  # Modelo de embeddings de Google

# Configuraci√≥n de Pinecone
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "codigo-penal-vertex-ai")
TOP_K_RESULTS = 10  # Aumentado para capturar m√°s contexto y encontrar art√≠culos espec√≠ficos

# --- INICIALIZACI√ìN DE SERVICIOS ---
print("üîß Inicializando Vertex AI y Pinecone...")

# Variable global para texto completo del PDF (para b√∫squeda exacta)
TEXTO_COMPLETO_PDF = None

try:
    # A. Inicializar Vertex AI
    vertexai.init(project=PROJECT_ID, location=REGION)
    print(f"‚úÖ Vertex AI inicializado - Proyecto: {PROJECT_ID}, Regi√≥n: {REGION}")
    
    # B. Inicializar Pinecone
    pc = Pinecone(api_key=PINECONE_API_KEY)
    PINECONE_INDEX = pc.Index(PINECONE_INDEX_NAME)
    print(f"‚úÖ Pinecone conectado - √çndice: {PINECONE_INDEX_NAME}")

    # C. Cargar Modelos de Vertex AI
    EMBEDDING_CLIENT = TextEmbeddingModel.from_pretrained(EMBEDDING_MODEL)
    LLM_CLIENT = GenerativeModel(MODEL_NAME)
    print(f"‚úÖ Modelos cargados - Embeddings: {EMBEDDING_MODEL}, LLM: {MODEL_NAME}")
    
    # D. Cargar texto completo del PDF para b√∫squeda exacta
    try:
        import PyPDF2
        pdf_path = "../documentos/codigo_penal.pdf"
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            texto_paginas = []
            for page in pdf_reader.pages:
                texto_paginas.append(page.extract_text())
            TEXTO_COMPLETO_PDF = "\n".join(texto_paginas)
            print(f"‚úÖ PDF cargado para b√∫squeda exacta ({len(TEXTO_COMPLETO_PDF)} caracteres)")
    except Exception as e:
        print(f"‚ö†Ô∏è No se pudo cargar PDF completo: {e} (b√∫squeda exacta deshabilitada)")
    
    print("‚úÖ ¬°Inicializaci√≥n completada con √©xito!")

except Exception as e:
    print(f"‚ùå ERROR DE INICIALIZACI√ìN: {e}")
    raise


# --- 2. MODELOS DE DATOS ---
class ChatRequest(BaseModel):
    pregunta: str


class ChatResponse(BaseModel):
    respuesta: str
    metadata: dict = None


# --- 3. INICIALIZAR LA APLICACI√ìN ---
app = FastAPI(
    title="API RAG - C√≥digo Penal Espa√±ol",
    description="API para consultas sobre el C√≥digo Penal usando RAG",
    version="1.0.0"
)

# Configurar CORS para permitir peticiones desde el frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En producci√≥n, especifica tu dominio exacto
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- 4. FUNCI√ìN CENTRAL DE RAG CON VERTEX AI ---

def buscar_articulo_exacto(texto_completo: str, numero_articulo: str) -> str:
    """
    Busca un art√≠culo espec√≠fico en el texto completo del PDF usando regex.
    Evita confusiones entre art. X y art. X bis/ter/quater.
    """
    import re
    
    # Patr√≥n mejorado: el PDF usa "Art√≠culo N \n" (espacio + salto), no punto
    # Busca "Art√≠culo 142 " (con espacio) pero NO "Art√≠culo 142 bis"
    pattern = rf"(?i)(art[√≠i]culo\s+{numero_articulo})\s+(?!bis|ter|quater)(.+?)(?=\n\s*Art[√≠i]culo\s+\d+\s|\Z)"
    
    match = re.search(pattern, texto_completo, re.DOTALL | re.IGNORECASE)
    
    if match:
        # Incluir el encabezado completo "Art√≠culo N"
        texto_articulo = match.group(0).strip()
        
        # Limitar a 2000 caracteres
        if len(texto_articulo) > 2000:
            texto_articulo = texto_articulo[:2000] + "\n\n[...texto truncado...]"
        
        return texto_articulo
    
    return None


def corregir_encoding(texto: str) -> str:
    """
    Corrige problemas de encoding comunes en el PDF del C√≥digo Penal
    """
    # Reemplazos b√°sicos de caracteres corruptos
    texto = texto.replace('√É¬≠', '√≠')
    texto = texto.replace('√É¬≥', '√≥')
    texto = texto.replace('√É¬±', '√±')
    texto = texto.replace('√É¬°', '√°')
    texto = texto.replace('√É¬©', '√©')
    texto = texto.replace('√É¬∫', '√∫')
    texto = texto.replace('√É¬º', '√º')
    texto = texto.replace('√É¬∂', '√∂')
    
    # May√∫sculas
    texto = texto.replace('√É', '√Å')
    texto = texto.replace('√É‚Ä∞', '√â')
    texto = texto.replace('√É"', '√ì')
    texto = texto.replace('√É≈°', '√ö')
    
    # Eliminar caracteres basura
    texto = texto.replace('√Ç', '')
    
    return texto


def generate_rag_response(query: str):
    """
    Sistema RAG h√≠brido con b√∫squeda exacta + vector search.
    
    1. Detecta si es consulta de art√≠culo espec√≠fico
    2. Intenta b√∫squeda exacta con regex primero
    3. Si no encuentra, usa RAG con embeddings
    4. Corrige encoding en todos los resultados
    """
    try:
        print(f"\n{'='*80}")
        print(f"üì® CONSULTA: {query}")
        print(f"{'='*80}")

        # --- PASO 1: DETECTAR N√öMERO DE ART√çCULO ---
        import re
        articulo_pattern = r'\b(?:art[√≠i]culo|art\.?)\s*(\d+)\b'
        solo_numero_pattern = r'^\s*(\d+)\s*$'
        
        numero_articulo = None
        match_articulo = re.search(articulo_pattern, query, re.IGNORECASE)
        match_numero = re.match(solo_numero_pattern, query)
        
        if match_articulo:
            numero_articulo = match_articulo.group(1)
            print(f"üéØ Art√≠culo detectado (patr√≥n completo): {numero_articulo}")
        elif match_numero:
            numero_articulo = match_numero.group(1)
            print(f"üéØ Art√≠culo detectado (solo n√∫mero): {numero_articulo}")
        
        # DEBUG: Verificar estado del PDF
        if numero_articulo:
            print(f"üìÑ TEXTO_COMPLETO_PDF disponible: {TEXTO_COMPLETO_PDF is not None}")
            if TEXTO_COMPLETO_PDF:
                print(f"üìÑ Tama√±o del PDF: {len(TEXTO_COMPLETO_PDF)} caracteres")
        
        # --- PASO 2: B√öSQUEDA EXACTA (si hay n√∫mero de art√≠culo y PDF cargado) ---
        if numero_articulo and TEXTO_COMPLETO_PDF:
            print(f"üîç Intentando b√∫squeda exacta para art√≠culo {numero_articulo}...")
            texto_exacto = buscar_articulo_exacto(TEXTO_COMPLETO_PDF, numero_articulo)
            
            if texto_exacto:
                print(f"‚úÖ ¬°Art√≠culo {numero_articulo} encontrado con b√∫squeda exacta!")
                texto_corregido = corregir_encoding(texto_exacto)
                
                # Responder directamente sin pasar por Gemini si es texto literal
                if len(texto_corregido) < 2000:  # Si es razonablemente corto
                    respuesta_final = f"**Art√≠culo {numero_articulo}**\n\n{texto_corregido}"
                    return {
                        "respuesta": respuesta_final,
                        "metadata": {
                            "num_fragmentos": 1,
                            "tiene_contexto": True,
                            "modelo": "B√∫squeda exacta (sin LLM)",
                            "embedding_model": "N/A",
                            "metodo": "exact_match"
                        }
                    }
                else:
                    # Si es muy largo, pasar por Gemini para formatear
                    prompt = f"""Eres un asistente legal especializado en el C√≥digo Penal espa√±ol.

El usuario pregunt√≥: "{query}"

Aqu√≠ est√° el texto LITERAL del art√≠culo encontrado:

{texto_corregido}

Instrucciones:
1. Responde con el texto COMPLETO del art√≠culo tal como aparece
2. NO resumas ni parafrasees - cita el texto literal
3. Si el art√≠culo es largo, pres√©ntalo de forma organizada pero completa
4. Usa formato Markdown para mejor legibilidad

Responde ahora:"""

                    response = LLM_CLIENT.generate_content(prompt)
                    return {
                        "respuesta": response.text,
                        "metadata": {
                            "num_fragmentos": 1,
                            "tiene_contexto": True,
                            "modelo": MODEL_NAME,
                            "embedding_model": "N/A",
                            "metodo": "exact_match_formatted"
                        }
                    }

        # --- PASO 3: ENRIQUECER QUERY (si no hubo match exacto) ---
        if numero_articulo:
            query_enriquecida = (
                f"Contenido literal del C√≥digo Penal espa√±ol "
                f"Art√≠culo {numero_articulo} delito pena castigo texto completo"
            )
            print(f"üîÑ Query enriquecida: {query_enriquecida}")
        else:
            query_enriquecida = query

        # --- PASO 4: GENERAR EMBEDDING ---
        print("üî¢ Generando embedding con Vertex AI...")
        embeddings = EMBEDDING_CLIENT.get_embeddings([query_enriquecida])
        query_vector = embeddings[0].values
        print(f"‚úÖ Embedding generado: {len(query_vector)} dimensiones")

        # --- PASO 5: B√öSQUEDA VECTORIAL EN PINECONE ---
        print(f"üîç Buscando en Pinecone (TOP_K={TOP_K_RESULTS})...")
        results = PINECONE_INDEX.query(
            vector=query_vector,
            top_k=TOP_K_RESULTS,
            include_metadata=True
        )

        # --- PASO 6: FILTRADO ADAPTATIVO ---
        umbral = 0.35 if numero_articulo else 0.45
        print(f"üìä Aplicando umbral adaptativo: {umbral}")
        
        contexto_parts = []
        for match in results['matches']:
            score = match.get('score', 0)
            print(f"  üìä Match con score: {score:.3f}")
            
            if score > umbral:
                text = match.get('metadata', {}).get('text', '')
                if text:
                    # Corregir encoding del texto recuperado
                    texto_corregido = corregir_encoding(text)
                    contexto_parts.append(f"[Fragmento del C√≥digo Penal - Relevancia: {score:.2f}]\n{texto_corregido}")
                    print(f"  ‚úì Fragmento aceptado (score: {score:.3f})")

        if not contexto_parts:
            print("‚ö†Ô∏è No hay resultados relevantes despu√©s del filtrado")
            return {
                "respuesta": "Lo siento, no encontr√© informaci√≥n relevante en el C√≥digo Penal sobre tu consulta. ¬øPodr√≠as reformularla o ser m√°s espec√≠fico?",
                "metadata": {
                    "num_fragmentos": 0,
                    "tiene_contexto": False,
                    "modelo": MODEL_NAME,
                    "embedding_model": EMBEDDING_MODEL,
                    "metodo": "rag_vector_search"
                }
            }

        contexto = "\n\n---\n\n".join(contexto_parts)
        num_matches = len(contexto_parts)
        print(f"üìã Contexto construido: {num_matches} fragmentos ({len(contexto)} caracteres)")

        # --- PASO 7: GENERAR RESPUESTA CON GEMINI ---
        prompt = f"""Eres un asistente jur√≠dico especializado en derecho penal espa√±ol. 
Tu tarea es proporcionar informaci√≥n del C√≥digo Penal espa√±ol bas√°ndote EXCLUSIVAMENTE en el contexto que te proporcionan.

PREGUNTA DEL USUARIO:
{query}

CONTEXTO RECUPERADO ({num_matches} fragmentos del C√≥digo Penal):
{contexto}

INSTRUCCIONES CR√çTICAS:
1. **Si el usuario menciona un n√∫mero de art√≠culo** (como "138", "art√≠culo 138", "art. 138"):
   - Busca ESE n√∫mero de art√≠culo en el contexto recuperado
   - Si lo encuentras, mu√©stralo COMPLETO con su texto literal
   - Usa el formato: **Art√≠culo [n√∫mero].** seguido del texto
   
2. **Si el contexto contiene el art√≠culo solicitado**: Mu√©stralo aunque sea parcial, indicando si est√° incompleto

3. **Si no encuentras el art√≠culo en el contexto**: Di claramente "El art√≠culo X no se encuentra en los fragmentos recuperados"

4. Para preguntas conceptuales (ej: "¬øQu√© es el homicidio?"):
   - Identifica el art√≠culo relevante en el contexto
   - Cita su contenido literal
   - A√±ade una explicaci√≥n breve

FORMATO DE RESPUESTA:
---
**Art√≠culo [n√∫mero].**
[Texto literal del C√≥digo Penal tal como aparece en el contexto]

üìò [Explicaci√≥n breve solo si es necesario]
---

RESPONDE AHORA bas√°ndote √∫nicamente en el contexto proporcionado:"""

        print("‚öñÔ∏è Generando respuesta con Gemini (Vertex AI)...")
        response = LLM_CLIENT.generate_content(prompt)
        
        print("‚úÖ Respuesta generada exitosamente")
        return {
            "respuesta": response.text,
            "metadata": {
                "num_fragmentos": num_matches,
                "tiene_contexto": True,
                "modelo": MODEL_NAME,
                "embedding_model": EMBEDDING_MODEL,
                "metodo": "rag_vector_search"
            }
        }

    except Exception as e:
        print(f"‚ùå Error en el proceso RAG: {e}")
        import traceback
        traceback.print_exc()
        return {
            "respuesta": f"Disculpa, ha ocurrido un error al consultar la base de datos de documentos: {str(e)}",
            "metadata": {
                "error": True,
                "mensaje_error": str(e),
                "num_fragmentos": 0,
                "tiene_contexto": False
            }
        }
# --- 5. ENDPOINT PRINCIPAL DE CHAT ---
@app.post("/chat", response_model=ChatResponse)
async def handle_chat_request(request: ChatRequest):
    """
    Endpoint principal que procesa la pregunta del usuario y devuelve una respuesta
    basada en el contexto del C√≥digo Penal usando Vertex AI.
    """
    pregunta_usuario = request.pregunta
    print(f"\n{'='*60}")
    print(f"ÔøΩ Nueva petici√≥n recibida")
    print(f"{'='*60}")
    
    # Llamar a la funci√≥n RAG con Vertex AI
    resultado = generate_rag_response(pregunta_usuario)
    
    return ChatResponse(
        respuesta=resultado["respuesta"],
        metadata={
            "pregunta": pregunta_usuario,
            "tieneContexto": resultado["metadata"].get("tiene_contexto", False),
            "numeroResultados": resultado["metadata"].get("num_fragmentos", 0),
            "modelo": resultado["metadata"].get("modelo", MODEL_NAME),
            "dominio": "codigo-penal-espanol",
            "proveedor": "Vertex AI (Google Cloud)"
        }
    )


# --- 6. ENDPOINT DE SALUD ---
@app.get("/health")
async def health_check():
    """Endpoint para verificar que la API est√° funcionando"""
    return {
        "status": "healthy",
        "service": "RAG API - C√≥digo Penal (Vertex AI)",
        "version": "2.0.0",
        "provider": "Google Cloud Vertex AI",
        "models": {
            "llm": MODEL_NAME,
            "embeddings": EMBEDDING_MODEL
        }
    }


# --- 7. ENDPOINT DE INFORMACI√ìN ---
@app.get("/")
async def root():
    """Informaci√≥n b√°sica de la API"""
    return {
        "message": "API RAG - C√≥digo Penal Espa√±ol (Vertex AI)",
        "version": "2.0.0",
        "provider": "Google Cloud Platform",
        "endpoints": {
            "chat": "/chat (POST)",
            "health": "/health (GET)",
            "docs": "/docs (Documentaci√≥n interactiva)"
        },
        "models": {
            "generacion": MODEL_NAME,
            "embeddings": EMBEDDING_MODEL
        }
    }

